{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd72433-f36f-4204-b614-3d3948a194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0401f123-0e31-4b4e-8872-6c080e56aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.read().split('\\n')\n",
    "    data = [line.split('\\t') for line in lines if line]\n",
    "    return data\n",
    "\n",
    "# 加载数据\n",
    "file_path = 'deu.txt'  # 替换为你的文件路径\n",
    "data = load_data(file_path)\n",
    "\n",
    "# 提取德语句子和英语句子\n",
    "de_sentences = [pair[1] for pair in data]\n",
    "en_sentences = [pair[0] for pair in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5d5eeb-30d8-4b35-8fa6-15c2fde30146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 转换为小写\n",
    "    sentence = sentence.lower()\n",
    "    # 去掉除空格外的所有标点符号\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "# 对德语和英语句子进行预处理\n",
    "de_sentences = [preprocess_sentence(sentence) for sentence in de_sentences]\n",
    "en_sentences = [preprocess_sentence(sentence) for sentence in en_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d499649-dbc7-4beb-8c0b-af8d7ea04632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "德语字符索引： {' ': 1, 'e': 2, 't': 3, 'o': 4, 'a': 5, 'i': 6, 'n': 7, 's': 8, 'h': 9, 'r': 10, 'd': 11, 'l': 12, 'm': 13, '.': 14, 'y': 15, 'u': 16, 'w': 17, 'g': 18, 'c': 19, 'f': 20, 'p': 21, \"'\": 22, 'b': 23, 'k': 24, 'v': 25, '?': 26, ',': 27, 'j': 28, 'x': 29, 'q': 30, 'z': 31, '\"': 32, '0': 33, '-': 34, '!': 35, '3': 36, '1': 37, '2': 38, ':': 39, '5': 40, '9': 41, '6': 42, '8': 43, '4': 44, '7': 45, '$': 46, '%': 47, '’': 48, 'é': 49, ';': 50, '/': 51, '\\xa0': 52, '₂': 53, '€': 54, '“': 55, '”': 56, '+': 57, '°': 58, '\\xad': 59, 'ü': 60, '—': 61, '(': 62, ')': 63, 'ï': 64, 'ñ': 65, 'ō': 66, '‘': 67, 'à': 68, '\\u200b': 69, 'ú': 70, 'â': 71, 'ç': 72, 'ê': 73, 'ã': 74, '@': 75, 'á': 76, '–': 77}\n",
      "英语字符索引： {' ': 1, 'e': 2, 'i': 3, 'n': 4, 't': 5, 's': 6, 'h': 7, 'r': 8, 'a': 9, 'c': 10, 'd': 11, 'm': 12, 'u': 13, 'l': 14, 'o': 15, '.': 16, 'g': 17, 'w': 18, 'b': 19, 'f': 20, 'k': 21, 'z': 22, ',': 23, 'v': 24, 'ü': 25, '?': 26, 'p': 27, 'ä': 28, 'ö': 29, 'ß': 30, 'j': 31, '!': 32, 'y': 33, '0': 34, '„': 35, '“': 36, 'x': 37, '1': 38, '-': 39, '3': 40, '’': 41, '2': 42, '\\xa0': 43, 'q': 44, '–': 45, '\"': 46, '9': 47, '5': 48, \"'\": 49, ':': 50, '8': 51, ';': 52, '6': 53, '4': 54, '7': 55, 'ō': 56, '—': 57, '\\u202f': 58, '%': 59, 'é': 60, '$': 61, '\\xad': 62, 'ū': 63, '(': 64, ')': 65, '‟': 66, '/': 67, '‚': 68, '‘': 69, '°': 70, '\\u200b': 71, '+': 72, '₂': 73, 'á': 74, '½': 75, 'í': 76, 'è': 77, 'û': 78, 'ñ': 79, 'à': 80, '€': 81, '”': 82, 'ű': 83, 'ˋ': 84, '‽': 85, 'ú': 86, 'ī': 87, 'ā': 88, 'ç': 89, 'ê': 90, 'ǎ': 91, '－': 92, '@': 93, 'ŏ': 94}\n"
     ]
    }
   ],
   "source": [
    "# 初始化字符级分词器\n",
    "de_tokenizer = Tokenizer(char_level=True)\n",
    "en_tokenizer = Tokenizer(char_level=True)\n",
    "\n",
    "# 拟合分词器\n",
    "de_tokenizer.fit_on_texts(de_sentences)\n",
    "en_tokenizer.fit_on_texts(en_sentences)\n",
    "\n",
    "# 获取字符索引\n",
    "de_char_index = de_tokenizer.word_index\n",
    "en_char_index = en_tokenizer.word_index\n",
    "\n",
    "# 打印字符索引\n",
    "print(\"德语字符索引：\", de_char_index)\n",
    "print(\"英语字符索引：\", en_char_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18fcfdf0-9008-4ea5-ad8d-ec9fb395b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大德语长度:472\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 76.2 GiB for an array with shape (131164552, 78) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m num_en_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(en_char_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 转换为one-hot向量\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m de_one_hot \u001b[38;5;241m=\u001b[39m \u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mde_padded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_de_chars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m en_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(en_padded_sequences, num_classes\u001b[38;5;241m=\u001b[39mnum_en_chars)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\src\\utils\\numerical_utils.py:98\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(x, num_classes)\u001b[0m\n\u001b[0;32m     96\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     97\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(batch_size), x] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    100\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 76.2 GiB for an array with shape (131164552, 78) and data type float64"
     ]
    }
   ],
   "source": [
    "# 将句子转换为字符序列\n",
    "de_sequences = de_tokenizer.texts_to_sequences(de_sentences)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(en_sentences)\n",
    "\n",
    "# 确定最大序列长度\n",
    "max_de_seq_length = max(len(seq) for seq in de_sequences)\n",
    "max_en_seq_length = max(len(seq) for seq in en_sequences)\n",
    "print(\"最大德语长度:{}\".format(max_de_seq_length))\n",
    "\n",
    "# 填充序列\n",
    "de_padded_sequences = pad_sequences(de_sequences, maxlen=max_de_seq_length, padding='post')\n",
    "en_padded_sequences = pad_sequences(en_sequences, maxlen=max_en_seq_length, padding='post')\n",
    "\n",
    "# 获取字符总数\n",
    "num_de_chars = len(de_char_index) + 1  # 加1是为了包含padding字符\n",
    "num_en_chars = len(en_char_index) + 1\n",
    "\n",
    "# 转换为one-hot向量\n",
    "de_one_hot = to_categorical(de_padded_sequences, num_classes=num_de_chars)\n",
    "en_one_hot = to_categorical(en_padded_sequences, num_classes=num_en_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf56d6-cdb0-4353-bd34-acdc354621df",
   "metadata": {},
   "source": [
    "~~暂时不知道为什么这么大，~~不过似乎暂时没法继续了。\n",
    "我猜是因为句子太多了，转换为的矩阵非常大；\n",
    "在机器翻译时，我们的输入实际上是三维向量（句子数（每60词分隔为一个句子），字符数（60），单词表）\n",
    "\n",
    "最大德语长度是472，然而据我所知，应该都是短句才对 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb2e9bba-5a6e-43fc-be31-9c78c1b22346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694f68f-2706-4616-b876-3cb88e9fe863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型参数\n",
    "latent_dim = 256  # LSTM单元数\n",
    "\n",
    "# 编码器\n",
    "encoder_inputs = Input(shape=(max_de_seq_length,))\n",
    "encoder_embedding = Embedding(input_dim=num_de_chars, output_dim=latent_dim, input_length=max_de_seq_length)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 解码器\n",
    "decoder_inputs = Input(shape=(max_en_seq_length,))\n",
    "decoder_embedding = Embedding(input_dim=num_en_chars, output_dim=latent_dim, input_length=max_en_seq_length)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_en_chars, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# 定义模型\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 打印模型结构\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4f431-f40d-4521-92cd-dcee3616efa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
